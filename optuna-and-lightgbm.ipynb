{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-27T13:46:47.039824Z","iopub.execute_input":"2023-10-27T13:46:47.040241Z","iopub.status.idle":"2023-10-27T13:46:47.515753Z","shell.execute_reply.started":"2023-10-27T13:46:47.040208Z","shell.execute_reply":"2023-10-27T13:46:47.514790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom path import Path\nfrom sklearn.model_selection import StratifiedKFold\nclass Config:\n    input_path = Path('../input/porto-seguro-safe-driver-prediction')\n    optuna_lgb = False\n    n_estimators = 1500\n    early_stopping_round = 150\n    cv_folds = 5\n    random_state = 0\n    params = {'objective': 'binary',\n              'boosting_type': 'gbdt',\n              'learning_rate': 0.01,\n              'max_bin': 25,\n              'num_leaves': 31,\n              'min_child_samples': 1500,\n              'colsample_bytree': 0.7,\n              'subsample_freq': 1,\n              'subsample': 0.7,\n              'reg_alpha': 1.0,\n              'reg_lambda': 1.0,\n              'verbosity': 0,\n              'random_state': 0}\n    \nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:46:47.517891Z","iopub.execute_input":"2023-10-27T13:46:47.519497Z","iopub.status.idle":"2023-10-27T13:46:52.702710Z","shell.execute_reply.started":"2023-10-27T13:46:47.519443Z","shell.execute_reply":"2023-10-27T13:46:52.701370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the training, test, and sample submission datasets; \n# set the index of the uploaded DataFrames to the identifier (the id column) of each data example\ntrain = pd.read_csv(config.input_path / 'train.csv', index_col='id')\ntest = pd.read_csv(config.input_path / 'test.csv', index_col='id')\nsubmission = pd.read_csv(config.input_path / 'sample_submission.csv', index_col='id')\ncalc_features = [feat for feat in train.columns if \"_calc\" in feat]\ncat_features = [feat for feat in train.columns if \"_cat\" in feat]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:46:52.704162Z","iopub.execute_input":"2023-10-27T13:46:52.704531Z","iopub.status.idle":"2023-10-27T13:47:05.969487Z","shell.execute_reply.started":"2023-10-27T13:46:52.704490Z","shell.execute_reply":"2023-10-27T13:47:05.967671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the target (a binary target of 0s and 1s) and remove it from the training dataset\ntarget = train[\"target\"]\ntrain = train.drop(\"target\", axis=\"columns\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:47:05.971270Z","iopub.execute_input":"2023-10-27T13:47:05.974814Z","iopub.status.idle":"2023-10-27T13:47:06.135198Z","shell.execute_reply.started":"2023-10-27T13:47:05.974740Z","shell.execute_reply":"2023-10-27T13:47:06.133839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop calc features\ntrain = train.drop(calc_features, axis=\"columns\")\ntest = test.drop(calc_features, axis=\"columns\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:47:06.139025Z","iopub.execute_input":"2023-10-27T13:47:06.139525Z","iopub.status.idle":"2023-10-27T13:47:06.277459Z","shell.execute_reply.started":"2023-10-27T13:47:06.139481Z","shell.execute_reply":"2023-10-27T13:47:06.276104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encoding the categorical features\ntrain = pd.get_dummies(train, columns=cat_features)\ntest = pd.get_dummies(test, columns=cat_features)\nassert((train.columns==test.columns).all())","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:47:06.279235Z","iopub.execute_input":"2023-10-27T13:47:06.279658Z","iopub.status.idle":"2023-10-27T13:47:08.860475Z","shell.execute_reply.started":"2023-10-27T13:47:06.279621Z","shell.execute_reply":"2023-10-27T13:47:08.859291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" A useful [link](https://pythonsimplified.com/difference-between-onehotencoder-and-get_dummies/) discussing the difference between OneHotEncoding and Pandas get_dummies ","metadata":{}},{"cell_type":"markdown","source":"Define evaluation metric - normalized Gini coefficient, code is from CPMP's [post](https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation)","metadata":{}},{"cell_type":"code","source":"from numba import jit\n\n@jit\ndef eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\ndef gini_lgb(y_true, y_pred):\n    eval_name = 'normalized_gini_coef'\n    eval_result = eval_gini(y_true, y_pred)\n    is_higher_better = True\n    return eval_name, eval_result, is_higher_better","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:47:08.862101Z","iopub.execute_input":"2023-10-27T13:47:08.862785Z","iopub.status.idle":"2023-10-27T13:47:10.482590Z","shell.execute_reply.started":"2023-10-27T13:47:08.862742Z","shell.execute_reply":"2023-10-27T13:47:10.481070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training params by Optuna; set the optuna_lgb flag to True in the Config class first; it takes around 4 mins per trial","metadata":{}},{"cell_type":"code","source":"if config.optuna_lgb:\n        \n    def objective(trial):\n        params = {\n    'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 1.0),\n    'num_leaves': trial.suggest_int(\"num_leaves\", 3, 255),\n    'min_child_samples': trial.suggest_int(\"min_child_samples\", \n                                           3, 3000),\n    'colsample_bytree': trial.suggest_float(\"colsample_bytree\", \n                                            0.1, 1.0),\n    'subsample_freq': trial.suggest_int(\"subsample_freq\", 0, 10),\n    'subsample': trial.suggest_float(\"subsample\", 0.1, 1.0),\n    'reg_alpha': trial.suggest_float(\"reg_alpha\", 1e-9, 10.0, log=True),\n    'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-9, 10.0, log=True),\n        }\n        \n        score = list()\n        skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, \n                              random_state=config.random_state)\n        for train_idx, valid_idx in skf.split(train, target):\n            X_train = train.iloc[train_idx]\n            y_train = target.iloc[train_idx]\n            X_valid = train.iloc[valid_idx] \n            y_valid = target.iloc[valid_idx]\n            model = lgb.LGBMClassifier(**params,\n                                    n_estimators=1500,\n                                    early_stopping_round=150,\n                                    force_row_wise=True)\n            callbacks=[lgb.early_stopping(stopping_rounds=150, \n                                          verbose=False)]\n            model.fit(X_train, y_train, \n                      eval_set=[(X_valid, y_valid)],  \n                      eval_metric=gini_lgb, callbacks=callbacks)\n              \n            score.append(\n                model.best_score_['valid_0']['normalized_gini_coef'])\n        return np.mean(score)\n    \n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=300)\n    print(\"Best Gini Normalized Score\", study.best_value)\n    print(\"Best parameters\", study.best_params)\n    \n    params = {'objective': 'binary',\n              'boosting_type': 'gbdt',\n              'verbosity': 0,\n              'random_state': 0}\n    \n    params.update(study.best_params)\n    \nelse:\n    params = config.params\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:47:10.484146Z","iopub.execute_input":"2023-10-27T13:47:10.484493Z","iopub.status.idle":"2023-10-27T13:47:10.500741Z","shell.execute_reply.started":"2023-10-27T13:47:10.484462Z","shell.execute_reply":"2023-10-27T13:47:10.498726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I manually stopped the run at trial 148 since the best trial is trial 77 with value 0.2884217536815791.\n\n>  Trial 147 finished with value: 0.28603083152555736 and parameters: {'learning_rate': 0.05304487091205322, 'num_leaves': 55, 'min_child_samples': 2774, 'colsample_bytree': 0.5997781187584867, 'subsample_freq': 4, 'subsample': 0.9578570860177447, 'reg_alpha': 0.34649717192668644, 'reg_lambda': 0.04990565230921181}. Best is trial 77 with value: 0.2884217536815791.","metadata":{}},{"cell_type":"markdown","source":"update params with these values\n> Trial 77 finished with value: 0.2884217536815791 and parameters: {'learning_rate': 0.034839525105479385, 'num_leaves': 27, 'min_child_samples': 2336, 'colsample_bytree': 0.5365060782638853, 'subsample_freq': 7, 'subsample': 0.9202881612831774, 'reg_alpha': 3.9121189920106114, 'reg_lambda': 0.0004977611982062695}. Best is trial 77 with value: 0.2884217536815791.","metadata":{}},{"cell_type":"code","source":"config.params.update({'learning_rate': 0.034839525105479385, 'num_leaves': 27, 'min_child_samples': 2336, 'colsample_bytree': 0.5365060782638853, 'subsample_freq': 7, 'subsample': 0.9202881612831774, 'reg_alpha': 3.9121189920106114, 'reg_lambda': 0.0004977611982062695})","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:47:10.503011Z","iopub.execute_input":"2023-10-27T13:47:10.503536Z","iopub.status.idle":"2023-10-27T13:47:10.524759Z","shell.execute_reply.started":"2023-10-27T13:47:10.503491Z","shell.execute_reply":"2023-10-27T13:47:10.523299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config.params","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:47:38.696677Z","iopub.execute_input":"2023-10-27T13:47:38.697108Z","iopub.status.idle":"2023-10-27T13:47:38.707154Z","shell.execute_reply.started":"2023-10-27T13:47:38.697068Z","shell.execute_reply":"2023-10-27T13:47:38.705817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" train a model on each cross-validation fold and use that fold to contribute to an average of test predictions; the model took around 10 mins to run.","metadata":{}},{"cell_type":"code","source":"preds = np.zeros(len(test))\noof = np.zeros(len(train))\nmetric_evaluations = list()\nskf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\nfor idx, (train_idx, valid_idx) in enumerate(skf.split(train, \n                                                       target)):\n    print(f\"CV fold {idx}\")\n    X_train, y_train = train.iloc[train_idx], target.iloc[train_idx]\n    X_valid, y_valid = train.iloc[valid_idx], target.iloc[valid_idx]\n    \n    model = lgb.LGBMClassifier(**params,\n                               n_estimators=config.n_estimators,\n                    early_stopping_round=config.early_stopping_round,\n                               force_row_wise=True)\n    \n    callbacks=[lgb.early_stopping(stopping_rounds=150), \n               lgb.log_evaluation(period=100, show_stdv=False)]\n                                                                                           \n    model.fit(X_train, y_train, \n              eval_set=[(X_valid, y_valid)], \n              eval_metric=gini_lgb, callbacks=callbacks)\n    metric_evaluations.append(\n                model.best_score_['valid_0']['normalized_gini_coef'])\n    preds += (model.predict_proba(test,  \n              num_iteration=model.best_iteration_)[:,1] \n              / skf.n_splits)\n    oof[valid_idx] = model.predict_proba(X_valid, \n                    num_iteration=model.best_iteration_)[:,1]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T13:47:50.478144Z","iopub.execute_input":"2023-10-27T13:47:50.478611Z","iopub.status.idle":"2023-10-27T13:57:59.015150Z","shell.execute_reply.started":"2023-10-27T13:47:50.478570Z","shell.execute_reply":"2023-10-27T13:57:59.013936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric_evaluations","metadata":{"execution":{"iopub.status.busy":"2023-10-27T14:08:58.395411Z","iopub.execute_input":"2023-10-27T14:08:58.395944Z","iopub.status.idle":"2023-10-27T14:08:58.404098Z","shell.execute_reply.started":"2023-10-27T14:08:58.395907Z","shell.execute_reply":"2023-10-27T14:08:58.402881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"print normalized gini coefficient","metadata":{}},{"cell_type":"code","source":"print(f'LightBGM CV normalized Gini coefficient:{np.mean(metric_evaluations):0.3f} ({np.std(metric_evaluations):0.3f})')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T14:05:03.389215Z","iopub.execute_input":"2023-10-27T14:05:03.389633Z","iopub.status.idle":"2023-10-27T14:05:03.396528Z","shell.execute_reply.started":"2023-10-27T14:05:03.389600Z","shell.execute_reply":"2023-10-27T14:05:03.395048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['target']=preds\nsubmission.to_csv('lgb_submission.csv')\n\noofs = pd.DataFrame({'id':train.index, 'target':oof})\noofs.to_csv('lgb_oof.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T14:13:28.744823Z","iopub.execute_input":"2023-10-27T14:13:28.746102Z","iopub.status.idle":"2023-10-27T14:13:35.464016Z","shell.execute_reply.started":"2023-10-27T14:13:28.746061Z","shell.execute_reply":"2023-10-27T14:13:35.462644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}